{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "WINDOW_OVERLAP = 2 # words of overlap between two windows\n",
    "DIFF_THRESH = WINDOW_OVERLAP + 1 # threshold for forming new cluster\n",
    "\n",
    "word_bag = defaultdict(list) # maps word to the list of clusters it belongs\n",
    "cluster_list = [] # list of all clusters\n",
    "\n",
    "tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, n):\n",
    "        self.words = defaultdict(int)\n",
    "        self.n = n\n",
    "        self.freq = 0\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        self.words[word] += 1\n",
    "        self.freq += 1\n",
    "        return self.update_set()\n",
    "    \n",
    "    def add_words(self, lst):\n",
    "        for word in lst:\n",
    "            self.words[word] += 1\n",
    "            self.freq += 1\n",
    "        return self.update_set()\n",
    "    \n",
    "    def update_set(self):\n",
    "        \"\"\"Maintains n words in the cluster\"\"\"\n",
    "        lst = []\n",
    "        while len(self.words) > self.n:\n",
    "            srt = sorted(self.words.items(), key=lambda kv: kv[1])\n",
    "            self.words.pop(srt[0][0])\n",
    "            lst += (srt[0][0])\n",
    "            self.freq -= srt[0][1]\n",
    "        return lst\n",
    "    \n",
    "    def get_words(self):\n",
    "        return set(self.words.keys())\n",
    "    \n",
    "    def get_topKwords(self, K):\n",
    "        sorted_dict = sorted(self.words.items(), reverse=True, key=operator.itemgetter(1))\n",
    "        K = min(K, len(sorted_dict))\n",
    "        word_list = []\n",
    "        total = 0\n",
    "        for i in range(K):\n",
    "            word_list += [sorted_dict[i][0]]\n",
    "            total += sorted_dict[i][1]\n",
    "        return (total, word_list)\n",
    "    \n",
    "    def get_word_freq(self, word):\n",
    "        return self.words[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Input and Processing and Window Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filename=''):\n",
    "    if filename:\n",
    "        return open(filename, 'r', encoding='utf-8').read()\n",
    "    else:\n",
    "        return \"Five score years ago, a great American, in whose symbolic shadow we stand today, signed the Emancipation Proclamation. \\\n",
    "This momentous decree came as a great beacon light of hope to millions of Negro slaves who had been seared in the flames of \\\n",
    "withering injustice. It came as a joyous daybreak to end the long night of their captivity. \\\n",
    "But one hundred years later, the Negro still is not free; one hundred years later, the life of the Negro is still sadly \\\n",
    "crippled by the manacles of segregation and the chains of discrimination; one hundred years later, the Negro lives on a \\\n",
    "lonely island of poverty in the midst of a vast ocean of material prosperity; one hundred years later, the Negro is still \\\n",
    "languished in the corners of American society and finds himself in exile in his own land\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = get_text()\n",
    "#text = get_text('darwin.txt')\n",
    "#text = get_text('mlk.txt')\n",
    "text = get_text('jupyter_usage_hindrances.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        if word.lower() not in sw:\n",
    "            cleaned = re.sub('[\\,.\\'\\(\\);\\/\\*:\\\"\\[\\]!]', '', word)\n",
    "            if cleaned:\n",
    "                words += [cleaned]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE + WINDOW TOKENIZER\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "#tokenizer.sent_end_chars = ('.', '?', '!', '--')\n",
    "for sentence in tokenizer.tokenize(text):\n",
    "    words = preprocess(sentence)\n",
    "    i = 0\n",
    "    n = len(words)\n",
    "    while i < n:\n",
    "        remain = (n - i)\n",
    "        if remain < (1.5*K):\n",
    "            div = int(math.ceil(remain / 2))\n",
    "            if div < (K/2):\n",
    "                tokens += [' '.join(words[i:])]\n",
    "            else:\n",
    "                tokens += [' '.join(words[i:i+div])]\n",
    "                tokens += [' '.join(words[i+div-WINDOW_OVERLAP:])]\n",
    "            i = n\n",
    "        else:\n",
    "            tokens += [' '.join(words[i:i+K])]\n",
    "            i += WINDOW_OVERLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_cluster(window):\n",
    "    b = None\n",
    "    thresh = DIFF_THRESH\n",
    "    chk_set = set(window)\n",
    "    for word in window:\n",
    "        for cluster in word_bag[word]:\n",
    "            # check overlap between window and cluster\n",
    "            chk = len(chk_set.intersection(cluster.get_words()))\n",
    "            if chk >= thresh:\n",
    "                b = cluster\n",
    "                thresh = chk\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMING CLUSTERS\n",
    "for token in tokens:\n",
    "    window = token.split()\n",
    "    cluster = best_cluster(window)\n",
    "    if not cluster: # if no best cluster found, create new cluster\n",
    "        cluster = Cluster(2*K)\n",
    "        cluster_list += [cluster]\n",
    "    removed = set(cluster.add_words(window))\n",
    "    for word in window:\n",
    "        if word in removed:\n",
    "            try:\n",
    "                word_bag[word].remove(cluster)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        elif cluster not in word_bag[word]:\n",
    "            word_bag[word] += [cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, ['editor', 'text', 'version', 'control', 'development', 'vim']) 71\n",
      "(46, ['version', 'control', 'code', 'better', 'git', 'editor']) 65\n",
      "(38, ['able', 'notebook', 'nice', 'sure', 'would', 'load']) 51\n",
      "(37, ['code', 'output', 'cells', 'content', 'easy', 'table']) 52\n",
      "(36, ['easy', 'way', 'code', 'cells', 'Would', 'push']) 50\n",
      "(34, ['##', '-', 'ENH', 'notebook', 'git', '&']) 49\n",
      "(33, ['run', '-', 'code', 'create', 'notebooks', 'cells']) 50\n",
      "(33, ['would', 'cells', 'use', 'multiple', 'copy', 'able']) 50\n",
      "(32, ['terminal', 'session', 'start', 'ipython', 'notebook', 'particular']) 49\n",
      "(31, ['editing', 'text', 'features', 'Lack', 'file', 'management']) 44\n",
      "(31, ['notebooks', 'version', 'control', 'use', 'via', 'UI']) 47\n",
      "(30, ['control', '-', 'code', 'version', 'make', 'files']) 47\n",
      "(30, ['use', 'Jupyter', 'Notebook', 'work', 'console', 'extensively']) 45\n",
      "(30, ['-', 'UBY', 'notebook', 'keyboard', '##1', 'shortcut']) 45\n",
      "(30, ['viable', 'workaround', 'httpsgithubcomjupyteratom-notebook', 'httpsgithubcomCacodaimonGhostText-for-Atom', 'projects', 'hopes']) 44\n"
     ]
    }
   ],
   "source": [
    "# FINDING TOP CLUSTERS\n",
    "sorted_clusters = sorted(cluster_list, reverse=True, key=lambda x: x.get_topKwords(6)[0])\n",
    "for cluster in sorted_clusters[:15]:\n",
    "    print(cluster.get_topKwords(6), cluster.freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
